{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT\n",
        "**BERT**, or <u>Bidirectional Encoder Representations from Transformers</u>, is a powerful natural language processing (NLP) technique developed by researchers at Google. It's based on the *Transformer architecture* and is designed to understand the context of words in a sentence by considering the words that come before and after them. Unlike previous models that processed words in a left-to-right or right-to-left manner, BERT can take into account the entire context of a word by processing it bidirectionally.\n",
        "\n",
        "BERT has achieved state-of-the-art results in various NLP tasks such as question answering, sentiment analysis, and language translation. It's pre-trained on large corpora of text data and can then be fine-tuned on specific tasks with smaller, task-specific datasets. This pre-training followed by fine-tuning approach has made BERT highly effective in a wide range of NLP applications."
      ],
      "metadata": {
        "id": "v2j11hVA-ilD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [Text preprocessing for BERT + SavedModel implementation of the encoder API](https://www.kaggle.com/models/tensorflow/bert/tensorFlow2/en-uncased-l-12-h-768-a-12)"
      ],
      "metadata": {
        "id": "GJ3C5bN9FPDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tensorflow2 BERT encoder model : **bert/tensorFlow2/en-uncased-l-12-h-768-a-12**\n",
        "\n",
        "Here, we are using \"BERT Base\" model having configuartion like\n",
        "\n",
        "**l-12-h-768-a-12**\n",
        "* l - 12 : layer = 12\n",
        "* h-768 : hidden state 768\n",
        "* a - 12 : attention 12"
      ],
      "metadata": {
        "id": "1g-H3701YYgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py\n",
        "!pip install typing-extensions\n",
        "!pip install wheel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knf990PQK7dz",
        "outputId": "8530f856-25ac-444f-deef-022cb08b9466"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (4.11.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.43.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow_text --use-deprecated=legacy-resolver\n",
        "# !pip install tf-keras --use-deprecated=legacy-resolver\n",
        "!pip3 install --quiet \"tensorflow-text==2.15.*\"\n",
        "!pip install tensorflow_hub\n",
        "# !pip install tf-keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWOmS3IGbt-n",
        "outputId": "e817a525-d4a1-495d-84f2-97407748941d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (3.20.3)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (2.15.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tf-keras>=2.14.1->tensorflow_hub) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (24.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow_hub) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/)"
      ],
      "metadata": {
        "id": "9t8xOCuydiPk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rg0LIXJb86cZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as tfhub\n",
        "import tensorflow_text as tftxt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_url = \"https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3\"\n",
        "bert_encoder_url = \"https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-l-12-h-768-a-12/4\""
      ],
      "metadata": {
        "id": "UyBA2Ju5cDK3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_model = tfhub.KerasLayer(bert_preprocess_url)"
      ],
      "metadata": {
        "id": "rSHjC_AVVW6Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_test = ['nice movie friend', 'I loved python programming']\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "# text_preprocessed is a dictionary, so let's take a look into keys\n",
        "text_preprocessed.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ9w-CMtV7TI",
        "outputId": "4a1dfa5d-b71f-4bec-95dc-074139582dc0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_word_ids', 'input_type_ids', 'input_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_preprocessed['input_mask']\n",
        "# the below first logic 1 is due to this\n",
        "# CLS nice movie friend SEP\n",
        "\n",
        "# here two (2) sentences and each is max length of 128 words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3CVcShGXtaH",
        "outputId": "720e14ad-cbbc-447f-f1e2-a3d9e8c968bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
              "array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_preprocessed['input_word_ids']\n",
        "# CLS nice movie friend SEP\n",
        "# 101 3835 3185  2767   102"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byNmUqxZZDnY",
        "outputId": "d4c69bf8-41d4-4c70-e8bf-e113b45bd047"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
              "array([[  101,  3835,  3185,  2767,   102,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [  101,  1045,  3866, 18750,  4730,   102,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = tfhub.KerasLayer(bert_encoder_url)\n",
        "bert_result = bert_model(text_preprocessed)\n",
        "bert_result.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rMzRgxGadp2",
        "outputId": "5ae8a2f7-af44-4737-d1fe-aa6a12161965"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['sequence_output', 'encoder_outputs', 'default', 'pooled_output'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generating Embedding vector of 768 length long for each word for 2 sentences"
      ],
      "metadata": {
        "id": "sOKrpwLrcXjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_result['pooled_output']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP21MBK3b129",
        "outputId": "76ec1d0e-a63e-4448-8d47-cd46af5c1330"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
              "array([[-0.8271397 , -0.27866328,  0.14801416, ...,  0.19063216,\n",
              "        -0.5919316 ,  0.8768732 ],\n",
              "       [-0.8536233 , -0.4849649 , -0.7472432 , ..., -0.6639452 ,\n",
              "        -0.6689642 ,  0.88667357]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Each word space vector of length 128 for 2 sentences and each word vector space is 768 long"
      ],
      "metadata": {
        "id": "ixe3p-yVchzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_result['sequence_output']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l50OS8HrcetQ",
        "outputId": "cf066337-4559-4b7a-aff4-79c5085a95fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 128, 768), dtype=float32, numpy=\n",
              "array([[[-0.09816232, -0.03544736, -0.19267285, ..., -0.15073586,\n",
              "          0.06901345,  0.02509328],\n",
              "        [ 0.4862165 , -0.65314376,  0.62446845, ..., -0.0665983 ,\n",
              "         -0.02937954, -0.0993511 ],\n",
              "        [ 0.6566795 , -0.73960876, -0.29676223, ..., -0.26809773,\n",
              "         -0.02502404, -0.42103842],\n",
              "        ...,\n",
              "        [ 0.13351414, -0.1494981 ,  0.47524282, ..., -0.05144007,\n",
              "         -0.11993749,  0.10318385],\n",
              "        [ 0.11948572, -0.14179072,  0.5047172 , ..., -0.08380911,\n",
              "         -0.11863765,  0.05432591],\n",
              "        [-0.08825113, -0.24356994,  0.35768592, ...,  0.08185339,\n",
              "         -0.09267313,  0.07827456]],\n",
              "\n",
              "       [[-0.09676915,  0.21864223, -0.3315135 , ..., -0.12111492,\n",
              "          0.38034782,  0.54683983],\n",
              "        [ 0.5134471 ,  0.0667595 , -0.26812196, ..., -0.08139098,\n",
              "          0.54236966,  0.32305327],\n",
              "        [ 1.1335996 ,  0.55607015,  0.36610845, ...,  0.32882318,\n",
              "          0.2631164 ,  0.15120265],\n",
              "        ...,\n",
              "        [ 0.07263805,  0.22572395,  0.20221777, ...,  0.17432213,\n",
              "         -0.004884  ,  0.32254064],\n",
              "        [ 0.07132921,  0.34484258,  0.23077585, ...,  0.06634276,\n",
              "          0.07765163,  0.43999574],\n",
              "        [-0.08915193, -0.01530895,  0.08247212, ...,  0.08373   ,\n",
              "         -0.03233793,  0.30324167]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### length of encoder in BERT Base model which 12 layer in length and each is of length of 768 vector long"
      ],
      "metadata": {
        "id": "bTxGygOPc4yW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(bert_result['encoder_outputs'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTC-wWVzdB-9",
        "outputId": "21b7394f-b1a3-4ab3-ef3f-8df91e90bec8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}