{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a8c8e42",
   "metadata": {},
   "source": [
    "# Quantization aware training in Keras example\n",
    "\n",
    "Credit : https://www.tensorflow.org/model_optimization/guide/quantization/training_example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a7a1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643247a5",
   "metadata": {},
   "source": [
    "# Train a model for MNIST without quantization aware training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806729c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mnist dataset\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "# (training dataset input, output), (testing dataset input, output)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4dea510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train) # input length of training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0266900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test) # input length of test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f185c16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0] # length of single vector in input dataset, will give same 60K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4a7be07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape # pixel matrix or shape of each training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59c8d3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape # matrix of entire input training dataset shape and shape of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0096ff03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1db077f83d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the image of single dataset, pixel of 28 x 28\n",
    "# plt.matshow(X_train[0].shape) # ValueError: not enough values to unpack (expected 2, got 1)\n",
    "plt.matshow(X_train[0]) # visualize single matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c299e40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the same with output dataset\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96d7b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize / Scale the input image so that each pixel value is between 0 to 1, since each pixel is of 256 (0 to 255)\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa7135e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0594fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f6c460e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1688/1688 [==============================] - 15s 9ms/step - loss: 0.2778 - accuracy: 0.9251 - val_loss: 0.1077 - val_accuracy: 0.9710\n",
      "Epoch 2/2\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.1058 - accuracy: 0.9692 - val_loss: 0.0724 - val_accuracy: 0.9795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1db03f19af0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7045ce",
   "metadata": {},
   "source": [
    "# Clone and fine-tune pre-trained model with quantization aware training\n",
    "Apply **quantization aware training** to the whole model and see this in the model summary. All layers are now prefixed by \"quant\".\n",
    "\n",
    "Note that the resulting model is **quantization aware** but not **_quantized_** <u>_(means the weights are float32 instead of int8)_</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "618a6883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLay  (None, 28, 28)           3         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " quant_reshape (QuantizeWrap  (None, 28, 28, 1)        1         \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_conv2d (QuantizeWrapp  (None, 26, 26, 12)       147       \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      " quant_max_pooling2d (Quanti  (None, 13, 13, 12)       1         \n",
      " zeWrapperV2)                                                    \n",
      "                                                                 \n",
      " quant_flatten (QuantizeWrap  (None, 2028)             1         \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_dense (QuantizeWrappe  (None, 10)               20295     \n",
      " rV2)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,448\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 38\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# q_aware_model, requires re-compile\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "                     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                     metrics=['accuracy'])\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece3df72",
   "metadata": {},
   "source": [
    "## Train and evaluate the model against baseline\n",
    "To demonstrate fine tuning after training the model for just an epoch, fine tune with quantization aware training on a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ccb59c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 28, 28)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_subset = X_train[0:1000] # out of 60000\n",
    "y_train_subset = y_train[0:1000]\n",
    "# y_train_subset.shape\n",
    "# X_train_subset\n",
    "# y_train_subset # first number is 5 (means first output number is '5')\n",
    "X_train_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a523f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 435ms/step - loss: 0.0922 - accuracy: 0.9778 - val_loss: 0.1142 - val_accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1db0671dfa0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm assuming train the model on flattened dataset\n",
    "q_aware_model.fit(X_train_subset, y_train_subset, batch_size=500, epochs=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d4334",
   "metadata": {},
   "source": [
    "### For this example, there is minimal to no loss in test accuracy after quantization aware training, compared to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84e5620e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.9765999913215637\n",
      "Quant test accuracy: 0.9767000079154968\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58edeb9",
   "metadata": {},
   "source": [
    "# Create quantized model for TFLite backend\n",
    "After this, you have an actually quantized model with int8 weights and uint8 activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8135f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as reshape_layer_call_fn, reshape_layer_call_and_return_conditional_losses, conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, flatten_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\39762\\AppData\\Local\\Temp\\tmp5idrmik3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\39762\\AppData\\Local\\Temp\\tmp5idrmik3\\assets\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452a330",
   "metadata": {},
   "source": [
    "# See persistence of accuracy from TF to TFLite\n",
    "Define a helper function to evaluate the TF Lite model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1e016da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(interpreter):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "    \n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for i, test_data in enumerate(X_test):\n",
    "        if(i % 1000) == 0:\n",
    "            print('Evaluated on {n} results so far.'.format(n=i))\n",
    "            # Pre-processing: add batch dimension and convert to float32 to match with the model's input data format.\n",
    "        test_data = np.expand_dims(test_data, axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_data)\n",
    "            \n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "            \n",
    "        # Post-processing: remove batch dimension and find the digit with highest probability.\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "    print('\\n')\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    accuracy = (prediction_digits == y_test).mean()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c9d844",
   "metadata": {},
   "source": [
    "##### You evaluate the quantized model and see that the accuracy from TensorFlow persists to the TFLite backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06377aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results so far.\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "\n",
      "\n",
      "Quant TFLite test_accuracy: 0.9767\n",
      "Quant TF test accuracy: 0.9767000079154968\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Quant TFLite test_accuracy:', test_accuracy)\n",
    "print('Quant TF test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c253dd4e",
   "metadata": {},
   "source": [
    "# See 4x smaller model from quantization\n",
    "##### You create a float TFLite model and then see that the quantized TFLite model is 4x smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a568082a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\39762\\AppData\\Local\\Temp\\tmpt1hw9yu4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\39762\\AppData\\Local\\Temp\\tmpt1hw9yu4\\assets\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 0.08062362670898438\n",
      "Quantized model in Mb: 0.023468017578125\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create float TFLite model.\n",
    "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "float_tflite_model = float_converter.convert()\n",
    "\n",
    "# Measure sizes of models.\n",
    "_, float_file = tempfile.mkstemp('.tflite')\n",
    "_, quant_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quant_file, 'wb') as f:\n",
    "  f.write(quantized_tflite_model)\n",
    "\n",
    "with open(float_file, 'wb') as f:\n",
    "  f.write(float_tflite_model)\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
